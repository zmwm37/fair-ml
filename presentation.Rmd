---
title: "Quantifying Fairness in Machine Learning"
author: "Zander Meitus"
date: "1/21/2021"
output:
  powerpoint_presentation: 
    reference_doc: deloitteTemplate.pptx
  slidy_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## What is Fair ML and why do we care?
One loose definition is "developing the benefits of machine learning for everyone." [google ML Fairness Gym] We care because the algorithms we develop for government clients affect U.S. citizens and the services their government provides them. 
  
Since our clients are massive government agencies, algorithms we build are inherently at risk of becoming what mathematician Cathy O'Neil describes as "Weapons of Math Destruction," which consist of three components:
  
- Opacity  
  
- Scale  
  
- Damage  
  
We can increase transparency, at a minimum with our clients, and decrease risk of damage by thinking about how our algorithms may affect groups differently.

## Mitigating Bias vs. Quantifying Bias
An excellent prior Analytics University talk from Faith Umoh & Daniel Beaulieu focused on ethically mitigating bias in algorithms. The talk covered two main types of bias (and techniques to avoid them):  

- Selection Bias: when the training sample contains a higher distribution of some sub-groups when compared to others, resulting in a misrepresentation of the population or situation that the model is seeking to understand    

- Unconcious Bias: the tendency to incorrectly design a model, collect input data, and interpret the results based on social stereotype about the groups associated with the problem  
  
While related, this presentation will talk about mathematical approaches to *quantify* algorithmic fairness, and how messy it can get.


## History
Like many things in math and computer science, what is "new" is often pretty old. Fair ML is a hot topic, but a very similar conversation was happening in the 1960's and 1970's. 
  
Two researches from Google Brain, Ben Hutchinson and Margaret Mitchell, published an [article](http://www.m-mitchell.com/papers/History_of_Fairness-arxiv.pdf) on this very topic.^[Google Brain has done great work in this space, but it's important to note the [recent pushing out](https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/) of one of their leading researchers, Timnit Gebru, over a forthcoming paper she had coauthored]  
  
- After the Civil Rights Act of 1964, "assessment tests in the public and private industry immediately came under public scrutiny"  
  
- "This stimulated a wealth of research into how to mathematically measure unfair bias and discrimination within the educational and employment testing communities, often with a focus on race"  

## Quantifying fair in different algorithm types
There are lots of classes of algorithms. The burgeoning field of fair ML is quickly demonstrating that different classes of algorithms require different quantitative frameworks for measuring fairness. Here are a few algorithm class examples and related fair ML research:  

- Regression  (ranking paper)  
  
- Classification ([Angwin et. al.](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), [Buolamwini et. al. 2018](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf))  
  
- Search Engines ([Algorithms of Opression by Safiya Noble](https://nyupress.org/9781479837243/algorithms-of-oppression/))  
  
- Recommendation Engines ([Diaz et. al. 2020](https://arxiv.org/pdf/2004.13157.pdf))  
  
- Natural Language Processing/Generation ([Sun et. al. 2019](https://www.aclweb.org/anthology/P19-1159.pdf),[Yeo & Chen 2020](https://arxiv.org/pdf/2008.01548.pdf))    
  
- Optimization (Corbett-Davies 2017)  
  
- Reinforcement Learning  ([Google ML Fairness Gym](https://github.com/google/ml-fairness-gym))

You could pick any combination of alogirthm classes above and they probably intersect or overlap. For today, we'll focus on classification. It's simpler (sort of) and probably the most dicsussed in "ML pop culture." 

## Classifation Models 
Classification models are a type of alogorithm that try to predict an outcome label for a given observation. We will be focusing on binary classification problems where there are only two possible outcomes.  
  
As a basic example, think of a model where we are trying to predict whether a patient is sick or not based on their temperature. Let's pick a cutoff of 98.6 degrees Farenheit
```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(knitr)
library(dplyr)
set.seed(303)

df <- data.frame(patient.id = 1:7, temp = round(rnorm(n = 7, mean = 98.6, sd =5),1)) %>% 
  mutate(pred.condition = ifelse(temp > 98.6, 'sick','healthy'),
    true.condition = c('healthy','sick','healthy','healthy','healthy', 'sick','sick'))

kable(df)
```
## Confusion Matrix  
A confusion matrix is a cross-tabulation of a classifier's predictions and the true values. The cells of the matrix, going clockwise from top left, represent true positives, false negatives, true negatives, and false positives. There are many metrics you can calculate from a confusion matrix, some of which we'll cover today.
![Confusion Matrix](confusion-matrix.JPG)

## Receiver-Operator Characterisitc (ROC) Curve
:::::::::::::: {.columns}
::: {.column}
True positive Rate = Number of true positives / Number of Condition Positive
- I.e., of all sick people, what proportion did we correctly identify as sick?  
  
False posite rate = Number of false positives / Number of Condition Negative
- I.e., of all healthy people, what proportion did we wrongly label as sick?  

:::
::: {.column}
![roc curve example](./roc-xkcd.jpg)
:::
::::::::::::::

## The Measure & Mismeasure of Fairness
In a 2018 paper, two researchers at Stanford University, Corbett-Davies and Goel, identified prevalent legal definitions of "fairness" in U.S. law and examined their statistical underpinnings and shortcomings. They identify the following three standards:  

- Anti-Classification: protected attributes are not explicitly used to make decisions  
  
- Classification Parity: common measures of predictive performance are equal across groups of protected attributes  
  
- Calibration: conditional on risk attributes, outcomes are independent of protected attributes     
  
Legal Vignette: Corbett-Davies and Goel discuss how U.S. law generally prohibits use of protected classes, which is in line with anti-classification. However, it is permitted in some cases like affirmative action. 

## Assumptions
Set of observable attributes for individual $i$: 
$$x_{i} \in \mathbb{R}^{p}$$

Two possible actions for a given individual: $a_{0}$ and $a_{1}$  
  
Decision algorithm is any function $d: R^{p} \rightarrow \{0,1\}$ where $d(x) = k$

## Assumptions
$x$ can be partitioned into protected and unprotected features
$$x = (x_{p}, x_{u})$$

For each individual, there is a quantity that specifies target of prediction
$$y \in \{0,1\}$$
Define random variables $X$ and $Y$ that take on values $X = x$ and $Y = y$ for an individual drawn randomly form population

True risk function 
$$r(x) = Pr(Y = 1 | X = x)$$

Risk assessment algorithms produce a risk score $s(x)$ that may be viewed as an approximation of true risk $r(x)$ which often has a threshold to convert from risk scores to decisions, setting $d(x) = 1$ if and only if $s(x) \geq t$ for some fixed threshold $t \in \mathbb{R}$

## COMPAS Data Overview
In 2016, ProPublica analyzed a commercially availbe algorithm made by Northpointe, Inc. that estimated the likelihood of a criminal defendant from become a recidivist (i.e. re-offending). Here's a brief description of the data

> We looked at more than 10,000 criminal defendants in Broward County, Florida, and compared their predicted recidivism rates with the rate that actually occurred over a two-year period. When most defendants are booked in jail, they respond to a COMPAS questionnaire. Their answers are fed into the COMPAS software to generate several scores including predictions of “Risk of Recidivism” and “Risk of Violent Recidivism.”

We'll focus on the the "Risk of Recidivism" score to apply the ideas in Corbett-Davies and Goel's work. The scores range from 1 (lowest risk) to 10 (highest risk) For a full description of the analysis, see [here](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)

## Anti-Classification

:::::::::::::: {.columns}
::: {.column}
Anti-classification: decision does not consider protected attributes  

$d(x) = d(x^{\prime})$ for all $x, x^{\prime}$ such that $x_{u} = x^{\prime}_{u}$

The COMPAS algorithm doesn't use race as an input, so a basic application of the risk score is essentially applying the "Anti-Classification" principle. Let's start with ROC curves for white and Black defendants.
:::
::: {.column}
```{r echo = FALSE, message = FALSE, warning = FALSE,  fig.width = 10}
library(tidyverse)
library(ROCR)

scores <- read.csv('../compas-analysis/compas-scores-two-years.csv')

# filter to white and Black only
scoresWB <- scores %>% 
  filter(race %in% c('African-American', 'Caucasian'))

# groups 
g <- unique(as.character(scoresWB$race))
lineColors <- c('red','blue')

# list for group values
outPerf <- list()

z <- 1
for(i in g) {
  # filter 
  s <- scoresWB %>% filter(race == i)
  
  # create prediction/performance
  pred <- prediction(s$decile_score, s$two_year_recid)
  rocPerf <- performance(pred, 'tpr','fpr')
  acc <- performance(pred,'acc')
  tnr <- performance(pred,'tnr')
  fnr <- performance(pred,'fnr')
  ppv <- performance(pred,'ppv')
  npv <- performance(pred,'npv')
  
  
  
  # create dataframe of cutoffs, tpr, and fpr
  outPerf[[z]] <- data.frame(race = i,
                             cutoff  = rocPerf@alpha.values[[1]],
                             tpr = rocPerf@y.values[[1]],
                             fpr = rocPerf@x.values[[1]],
                             acc = acc@y.values[[1]],
                             tnr = tnr@y.values[[1]],
                             fnr = fnr@y.values[[1]],
                             ppv = ppv@y.values[[1]],
                             npv = npv@y.values[[1]])
  
  # if not first curve, add to existing plot
  b <- if(z > 1) {
    TRUE
  } else {
    FALSE
  }
  l <- if(z > 1){
    2
  } else {
    1
  }
  
  # plot
  plot(rocPerf, add = b, lwd = 3, lty = l, col = lineColors[z])
  text(x = 0.6, y = 0.5-(z*.05), labels = paste(g[z],' AUC: ', round(performance(pred, 'auc')@y.values[[1]],3)))
  
  if (z == 1) {
    legend('bottomright',
           legend = g,
           col = lineColors,
           lty = c(1,2))
  }
  
  z <- z + 1
} 
test <- performance(pred,'auc')

```
:::
::::::::::::::


## Anti-Classification Limitations
:::::::::::::: {.columns}
::: {.column}
Ok, curves look pretty similar and the areas under the curve (AUCs) are comparable. If we look at errror rates (false positive rate and false negative rate) by race, we see some discrepancies. At any given cutoff Caucasians have a higher false negative rate and African-Americans have a higher false positive rate.
:::
::: {.column}
```{r, echo = FALSE, message = FALSE, warning = FALSE,  fig.width = 12}
library(gridExtra)
# bind group dataframes 
perfDf <- do.call(rbind,outPerf)

# plot fpr comparing groups
p1 <- ggplot(data = perfDf, aes(x = as.factor(cutoff),
                                y = fpr, 
                                group = race,
                                linetype = race, 
                                color = race)) + 
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4)+
  theme(legend.position = 'none',
        text = element_text(size = 20)) +  
  ylab('False Positive Rate')+
  xlab('Model Cutoff')+
  ggtitle('False Positive Rate by Cutoff')

# plot fnr comparing groups
p2 <- ggplot(data = perfDf, aes(x = as.factor(cutoff),
                                y = fnr, 
                                group = race,
                                linetype = race,
                                color = race)) +
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4)+
  theme(legend.position = c(.75,.15),
        text = element_text(size = 20))+
  ylab('False Negative Rate')+
  xlab('Model Cutoff')+
  ggtitle('False Negative Rate by Cutoff')

grid.arrange(p1,p2, ncol = 2)
```
:::
::::::::::::::

## Anti-Classification
:::::::::::::: {.columns}
::: {.column}
But there are other metrics to consider in a confusion matrix. In Northpointe's rebuttal to ProPublica, they argue that COMPAS achieves predictive parity since the positive and negative predictive values are comparable. (I don't have a Ph.D. like the Northpointe authors, but the curves don't look comparable to me...)
  
As we will discuss, confusion matrix metrics on their own are not conclusive for a model's fairness, but rather a diagnostic starting point.
:::
::: {.column}
```{r,echo = FALSE, message = FALSE, warning = FALSE,  fig.width = 12}
# plot fpr comparing groups
p3 <- ggplot(data = perfDf, aes(x = as.factor(cutoff),
                                y = ppv, 
                                group = race,
                                linetype = race, 
                                color = race)) + 
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4)+
  theme(legend.position = 'none',
        text = element_text(size = 20)) +  
  ylab('Positive Predictive Value')+
  xlab('Model Cutoff')+
  ggtitle('Positive Predictive Value by Cutoff')+
  ylim(0,1)

# plot fnr comparing groups
p4 <- ggplot(data = perfDf, aes(x = as.factor(cutoff),
                                y = npv, 
                                group = race,
                                linetype = race,
                                color = race)) +
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4)+
  theme(legend.position = c(.25,.15),
        text = element_text(size = 20))+
  ylab('Negative Predictive Value')+
  xlab('Model Cutoff')+
  ggtitle('Negative Predictive Value by Cutoff') +
  ylim(0,1)

grid.arrange(p3,p4, ncol = 2)
```
:::
::::::::::::::
## Classification Parity
Classification parity: some given measure of classification error is equal across groups defined by protected attributes. There are lots of metrics from confusion matrix that could be used here (Berk et. al.), as well as the Area Under the Curve. Corbett-Davies highlight:  
  
Demographic parity: parity in proportion of positive decisions  
$$Pr(d(X) = 1 | X_{p}) = Pr(d(X) = 1)$$
  
Parity of false positive rates  
$$Pr(d(X)  = 1 | Y = 0, X_{p}) = Pr(d(X) = 1 | Y = 0)$$

## Classification Parity
:::::::::::::: {.columns}
::: {.column}
For a simple demonstration of classification parity, let's take the error curves from the prior example. If we shift the curves for African-Americans to the left by 2, they become very comparable at several risk scores to the curves for Caucasians. So by subtracting 2 from the risk scores for African-Americans, the model becomes "fairer" based on false positive and false negative rates according to classification parity... but there is more to it.
:::
::: {.column}
```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.width = 12}

perfDf <- perfDf %>%
  mutate(cutoff2 = ifelse(race == 'African-American',cutoff-2, cutoff))

# plot fpr comparing groups
p5 <- ggplot(data = perfDf, aes(x = as.factor(cutoff2), 
                                y = fpr, 
                                group = race, 
                                color = race,
                                linetype = race)) + 
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4)+
  theme(legend.position = 'none',
        text = element_text(size = 20)) +
  ylab('False Positive Rate') +
  xlab('Modified Model Cutoff') + 
  ggtitle('Classification Calibration: FPR')

# plot fnr comparing groups
p6<- ggplot(data = perfDf, aes(x = as.factor(cutoff2), 
                               y = fnr, 
                               group = race, 
                               color = race,
                               linetype = race)) + 
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4)+
  theme(legend.position = c(0.75,0.15),
        text = element_text(size = 20))+
  ylab('False Negative Rate') +
  xlab('Modified Model Cutoff') + 
  ggtitle('Classification Calibration: FNR')

grid.arrange(p5,p6, ncol = 2)
```
:::
::::::::::::::

## Limitations of Classification Parity
:::::::::::::: {.columns}
::: {.column}
Corbett-Davies et. al. point out that when your model does not meet classification parity (any kind), it does not *necessarily* mean your model is not fair. Rather, it says different groups have different estimated risk rates. So now you have to ask "why?" Is it because:   
- These groups truly have different risk rates (this answer deserves high scrutiny!)  
- The data feeding into your model are not of equal quality across groups (back to Selection & Unconcious Bias)  
- The model hasn't properly captured the relationship between the features and risk  
:::
::: {.column}
![Estimated Risk Distributions of Violent Recidivism](violent-recid-dist.JPG)

:::
:::::::::::::: 

## Calibration
:::::::::::::: {.columns}
::: {.column}
Calibration: outcomes should be independent of protected attributes conditional on risk score. Given risk scores $s(x)$:
$$Pr(Y = 1 | s(X), X_{p}) = Pr(Y =1 | s(X))$$
At several model cutoffs, the chart below shows that the model is closely calibrated between white and Black defendants.
:::
::: {.column}
```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.width= 8}
cal <- scoresWB %>%
  group_by(decile_score, race) %>%
  summarize(n = n(),
         recid.count = sum(two_year_recid),
         recid.rate = recid.count / n )

ggplot(cal, aes(x = as.factor(decile_score), 
                y = recid.rate, 
                group = race, 
                linetype = race,
                color = race)) +
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4) +
  ylab("Two Year Recidivism Rate")+
  xlab("Model Cutoff")+
  ggtitle("Recidivism Rates by Model Cutoff")+
  ylim(0,1) +
  theme(legend.position = c(0.75,0.2),
        text = element_text(size = 20))
```
:::
::::::::::::::

## Calibration Limitations
:::::::::::::: {.columns}
::: {.column}
Corbett-Davies et. al. present redlining as a perfect example of how you can satisfy calibration, but not acheive fairness. Imagine a scenario with two neighborhoods (1 & 2), each with two groups (A & B). Within each neighborhood, risk distributions are the same across groups. A loan classifier model is built exlusively on neighborhood. It rates all neighborhood 1 as "denied" and all neighborhood 2 as "approved." In this example, the model is ignored and everyone is given a loan. 
:::
::: {.column}
```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.width = 12}
set.seed(33)
n<- 1000
s <- .1
d <- data.frame(group = rep(c('A','B','A','B'),c(n*s,n*(1-s),n*(1-s),n*s)), 
                neighborhood = rep(c('neighborhood1','neighborhood2'),c(n,n)),
                default.risk = c(round(rnorm(n*s,0.5,.12),2),
                                 round(rnorm(n*(1-s),0.5,.12),2),
                                 round(rnorm(n*(1-s),0.33,.12),2),
                                 round(rnorm(n*s,0.33,.12),2)),
                loan = rep(c('Denied','Approved'),c(n,n))
                )

g1 <- ggplot(d, aes(x = default.risk, fill = group))+
  geom_density(alpha = 0.5, aes(y = ..count..))+
  facet_grid(rows = vars(neighborhood))+
  xlim(0,1)+
  theme(legend.position = 'none',
        text = element_text(size = 20)) +
  ggtitle('Risk Distribution')

dSum <- d %>% 
  group_by(loan, group) %>%
  summarize(default.rate = mean(default.risk),
            n = n())

g2 <- ggplot(dSum, aes(x = loan, y = default.rate, fill = group)) +
  geom_bar(stat = 'identity', position = 'dodge')+
  theme(legend.position = c(0.15,0.85),        
        text = element_text(size = 20)) +
  ggtitle('Default Rates')


g3 <- ggplot(dSum, aes(x = loan, y = n, fill = group)) +
  geom_bar(stat = 'identity', position = 'dodge')+
  theme(legend.position = 'none',
        text = element_text(size = 20)) +
  ggtitle('Loan Status Counts')


grid.arrange(g1,g2,g3, ncol = 3)

```
:::
::::::::::::::


## Double Edge Data Sword
In order to evaluate models for their impact on certain sub-groups, your data needs to contain observations of whatever variable defines these sub-groups. 

These data can be powerful and helpful when applied to making algorithms fairer, but they can also be used for nefarious discriminatory purposes. Always think carefully about what data is actually necessary, how the data is collected, who can access the data, and how the data could be misused.  

## Available Tools
MIT CS Ph.D. student Julius Adebayo https://github.com/adebayoj/fairml 

Google ML Fairness Gym




## Bibliography
Angwin, Julia, Jeff Larson, Surya Mattu and Lauren Kirchner. "Machine Bias." *ProPublica* May 23, 2016. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing (Accessed January 17, 2021)  
  
Bolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. "Man is to Computer Programmer as Woman is to Homemaker?: Debiased Word Embeddings." *NIPS '16: Proceedings of the 30th International Conference on Neural Information Processing Systems* (December 2016): 4356-4364. https://dl.acm.org/doi/10.5555/3157382.3157584 (Accessed January 17, 2021).

Buolamwini, Joy and Timnit Gebru. "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification." *Proceedings of Machine Learning Research* 81 (2018): 77-91. http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf (Accessed January 17, 2021).   

Corbett-Davies, Sam and Sharad Goel. "The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning." (August 2018). https://arxiv.org/pdf/1808.00023.pdf (Accessed January 17, 2021).
  
Diaz, Fernando, Bhaskar Mitra, Michael D. Ekstrand, Asia J. Biega, and Ben Carterette. "Evaluating Stochastic Rankings with Expected Exposure." *Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM ’20)* (October 2020). https://doi.org/10.1145/3340531.3411962 (Accessed January 17, 2021).    
    
Hutchinson, Ben and Margaret Mitchell. "50 Years of Testing (Un)Fairness: Lessons for Machine Learning." *FAT '19: Proceedings of the Conference on Fairness, Accountability, and Transparency* (January 2019): 49-58.https://arxiv.org/pdf/1811.10104.pdf (Accessed January 17, 2021).   
   
O'Neil, Cathy. *Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy.* New York: Crown Publishing Group, 2016.  
  
Sun, Tony, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei chang, and William Yang Wang. "Mitigating Gender Bias in Natural Language Processing: Literature Review." *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics* (2019): 1630-1640. https://www.aclweb.org/anthology/P19-1159.pdf (Accessed January 17, 2021).    

Yeo, Catherine and Alyssa Chen. "Defining and Evaluating Fair Natural Language Generation." *Proceedings of the Fourth Widening Natural Language Processing Workshop* (July 2020). https://www.aclweb.org/anthology/2020.winlp-1.27/ (Accessed January 17, 2021).  

