---
title: "fairML"
author: "Zander Meitus"
date: "1/15/2021"
output:
  slidy_presentation: default
  powerpoint_presentation:
    reference_doc: deloitteTemplate.pptx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Framing the Problem


### What is Fair ML and why do we care?


### Mitigating Bias vs. Quantifying Bias
Prior talk focused on ethically mitigating bias in algorithms. This talk focuses on techniques to quanitfy bias.

### Defintions
Ok great, let's quantify bias or fairness. What do you mean by "fairness" in Machine Learning?  
- This presentation will talk about approaches and challenges to mathematically defining fairness  
- Ultimately, fairness includes a philosohpical component as well, but as data science practitioners working in quantitative methods, it's helpful to apply a mathematical framework  

## History
Like many things in math and computer science, what is "new" is often pretty old. Fair ML is a hot topic, but a very similar conversation was happening in the 1960's and 1970's. [50 years of (Un)Fair Paper]

## Quantifying fair in different algorithm types
There are lots of classes of algorithms. The burgeoning field of fair ML is quickly demonstrating that differently classes of algorithms require different quantitative frameworks for measuring fairness. Here are a few algorithm class examples and related fair ML research:  
- regression  (ranking)  
- classification (ProPublica, Buolamwini & Gebru)  
- recommendation engines (Algorithms of Opression for non-technical dicussion of what's unfair)  
- natural language processing/generation (Yeo & Chen)    
- Optimization  

You could pick and combination of bulletpoints above and they probably intersect or overlap. For today, we'll focus on classification. It's simpler (sortof) and probably the most dicsussed in "ML pop culture."

## Corbett-Davies & Goyle: [Paper title]
In a 2018 paper, two researchers at Stanford University, Corbett-Davies and Goyle, identified prevalent legal definitions of "fairness" in U.S. law and examined their statistical underpinnings and shortcomings. They identify the following three standards:  
- anti-classification: [definition]  
- classification parity: [definition]  
- calibration: [definition]   

## Legal Vignette
Corbett-Davies discuss how U.S. law generally prohobits use of protected classes, which is in line with anti-classification. However, it is permitted in some cases like affirmative action. 

## formulas
ADD SOME EXAMPLES HERE
set of observable attributes for individual $i$: 
$$x_{i} \in \mathbb{R}^{p}$$

Two possible actions: $a_{0}$ and $a_{1}$

Decision algorithm is any function $d: R^{p} \rightarrow \{0,1\}$ where $d(x) = k$

## assumptions - corbett davies
- x can be partitioned into protected and unprotected features
$$x = (x_{p}, x_{u})$$

- For each individual, there is a quantity that specifies target of prediction
$$y \in \{0,1\}$$
- Define random variables $X$ and $Y$ that take on values $X = x$ and $Y = y$ for and individual drawn randomly form population

- True risk function 
$$r(x) = Pr(Y = 1 | X = x)$$

- Risk assessment algorithms produce a risk score $s(x)$ that may be viewed as an approximation of true risk $r(x)$ which often has a threshold to convert from risk scores to decisions, setting $d(x) = 1$ if and only if $s(x) \geq t$ for some fixed threshold $t \in \mathbb{R}$

## COMPAS Data Overview
ProPublica 

## Mathematical definitions: Anti-Classification
Anti-classification: decision do not consider protected attributes  
$d(x) = d(x^{\prime})$ for all $x, x^{\prime}$ such that $x_{u} = x^{\prime}_{u}$

The COMPAS algorithm doesn't use race as an input, so a basic application of the risk score is essentially applying the "Anti-Classificatio" principle. 

There are several quantitative techniques that we could use to compare outcomes across groups.[source?] Let's start with ROC curves for white and Black defendants.
[Insert ROC Curve]

Ok, curves look pretty similar and the areas under the curve (AUCs) are comparable. But this is a first example of nuance for measuring fariness. If we look at errror rates (false positive rate and false negative rate) by race, we see some discrepancies. At any given cutoff Caucasians have a higher false negative rate and African-Americans have a higher false positive rate. If we think of the implications of these errors, it means that at any given model score cutoff:  
- point 1  
- point 2  

## Mathematical definitions: Classification Parity
Classification parity: some given measure of classification error is equal across groups defined by protected attributes. There are lots of metrics from confusion matrix that could be used here (Berk et. al.), as well as the Area Under the Curve. Corbett-Davies highlight:
- demographic parity: parity in proportion of positive decisions  
$$Pr(d(X) = 1 | X_{p}) = Pr(d(X) = 1)$$  
- parity of false positive rates  
$$Pr(d(X)  =1 | Y = 0, X_{p}) = Pr(d(X) = 1 | Y = 0)$$  

## Mathematical definitions: Calibration
Calibration: outcomes should be independent of protected attributes conditional on risk score. Give risk scores $s(x)$
$$Pr(Y = 1 | s(X), X_{p}) = Pr(Y =1 | s(X))$$

## Bibliography


