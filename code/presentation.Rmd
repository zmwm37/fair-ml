---
title: "Quantifying Fairness in Machine Learning"
author: "Zander Meitus"
date: "1/28/2021"
output:
  powerpoint_presentation: 
    reference_doc: deloitteTemplate.pptx
  slidy_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## What is Fair ML?         Why do we care?
One loose definition is "developing the benefits of machine learning for everyone." ^[As defined on the [FAQ page of Google's ML Fairness Gym](https://github.com/google/ml-fairness-gym/blob/master/docs/FAQ.md)] We care because the algorithms we develop for government clients affect U.S. citizens and the services their government provides them. 
  
Since our clients are massive government agencies, algorithms we build are inherently at risk of becoming what mathematician Cathy O'Neil describes as "Weapons of Math Destruction," which consist of three components:
  
- **Opacity**  
  
- **Scale**  
  
- **Damage**  
  
We can increase transparency, at a minimum with our clients, and decrease risk of damage by thinking about how our algorithms may affect groups differently.

## Mitigating vs. Quantifying Bias
An excellent prior Analytics University talk from Faith Umoh & Daniel Beaulieu focused on ethically mitigating bias in algorithms. The talk covered two main types of bias (and techniques to avoid them):  

- **Selection Bias:** when the training sample contains a higher distribution of some sub-groups when compared to others, resulting in a misrepresentation of the population or situation that the model is seeking to understand    

- **Unconcious Bias:** the tendency to incorrectly design a model, collect input data, and interpret the results based on social stereotype about the groups associated with the problem  
  
While related, this presentation will talk about mathematical approaches to *quantify* algorithmic fairness, and how messy it can get.


## Some History
Like many things in math and computer science, what is "new" is often pretty old. Fair ML is a hot topic, but a very similar conversation was happening in the 1960's and 1970's. 
  
Two researches from Google Brain, Ben Hutchinson and Margaret Mitchell, published an [article](http://www.m-mitchell.com/papers/History_of_Fairness-arxiv.pdf) on this very topic.^[Google Brain has done great work in this space, but it's important to note the [recent pushing out](https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/) of one of their leading researchers, Timnit Gebru, over a forthcoming paper she had coauthored]  
  
- After the Civil Rights Act of 1964, "assessment tests in the public and private industry immediately came under public scrutiny."  
  
- "This stimulated a wealth of research into how to mathematically measure unfair bias and discrimination within the educational and employment testing communities, often with a focus on race."  

## "Fair" Across Algorithms
There are lots of classes of algorithms. The burgeoning field of fair ML is quickly demonstrating that different classes of algorithms require different quantitative frameworks for measuring fairness. Here are a few algorithm class examples and related fair ML research:  
  
- **Classification** ([Angwin et. al.](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), [Buolamwini et. al. 2018](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf))  
  
- **Search Engines & Recommendation Engines** ([Algorithms of Opression by Safiya Noble](https://nyupress.org/9781479837243/algorithms-of-oppression/), [Diaz et. al. 2020](https://arxiv.org/pdf/2004.13157.pdf))    
  
- **Natural Language Processing/Generation** ([Sun et. al. 2019](https://www.aclweb.org/anthology/P19-1159.pdf),[Yeo & Chen 2020](https://arxiv.org/pdf/2008.01548.pdf))    
  
- **Graph & Network Analysis** ([Atwood et. al. 2019](https://arxiv.org/abs/1911.05489))
  
- **Reinforcement Learning**  ([D'Amour et. al. 2020](https://dl.acm.org/doi/abs/10.1145/3351095.3372878))

You could pick any combination of algorithm classes above and they probably intersect or overlap. For today, we'll focus on classification. It's simpler (sort of) and probably the most dicsussed in "ML pop culture." 

## Classification Models 
:::::::::::::: {.columns}
::: {.column}
Classification models are a type of algorithm that try to predict an outcome label for a given observation. We will be focusing on binary classification problems where there are only two possible outcomes. As a basic example, think of a model where we are trying to predict whether a patient is sick or not based on their temperature. Let's pick a cutoff of 98.6 degrees Fahrenheit.
:::
::: {.column}
```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(knitr)
library(dplyr)
set.seed(303)

df <- data.frame(patient.id = 1:7, temp = round(rnorm(n = 7, mean = 98.6, sd =5),1)) %>% 
  mutate(pred.condition = ifelse(temp > 98.6, 'sick','healthy'),
    true.condition = c('healthy','sick','healthy','healthy','healthy', 'sick','sick'))

kable(df)
```
:::
::::::::::::::
## Confusion Matrix  
:::::::::::::: {.columns}
::: {.column}
![Source: [Wikipedia "Confusion Matrix" Page ](https://en.wikipedia.org/wiki/Confusion_matrix)](../images/confusion-matrix.JPG)
:::
::: {.column}
A confusion matrix is a cross-tabulation of a classifier's predictions and the true values. The cells of the matrix, going clockwise from top left, represent true positives, false negatives, true negatives, and false positives. There are many metrics you can calculate from a confusion matrix, some of which we'll cover today.
:::
::::::::::::::


## Receiver-Operator Characteristic (ROC) Curve
:::::::::::::: {.columns}
::: {.column}
![Source: [Wikipedia user MartinThoma](https://en.wikipedia.org/wiki/File:Roc-draft-xkcd-style.svg)](../images/roc-xkcd.jpg)
:::
::: {.column}
True Positive Rate = Number of True Positives / Number of Condition Positive
- I.e., of all sick people, what proportion did we correctly identify as sick?  
  
False Positive Rate = Number of False Positives / Number of Condition Negative
- I.e., of all healthy people, what proportion did we wrongly label as sick?  
:::
::::::::::::::

## The Measure & Mismeasure of Fairness
In a 2018 paper, two researchers at Stanford University, Sam Corbett-Davies and Sharad Goel, identified prevalent definitions of "fairness" for risk assessment tools and examined their statistical underpinnings and shortcomings. They identify the following three standards:  

- **Anti-Classification:** protected attributes are not explicitly used to make decisions  
  
- **Classification Parity:** common measures of predictive performance are equal across groups of protected attributes  
  
- **Calibration:** conditional on risk attributes, outcomes are independent of protected attributes     
  
Legal Vignette: Corbett-Davies and Goel discuss how U.S. law often prohibits use of protected classes in many types of decisions, which is in line with anti-classification. However, it is permitted in some cases like affirmative action. 

## Assumptions
Set of observable attributes for individual $i$: 
$$x_{i} \in \mathbb{R}^{p}$$  
  
Two possible actions for a given individual: $a_{0}$ and $a_{1}$    
    
Decision algorithm is any function $d: R^{p} \rightarrow \{0,1\}$ where $d(x) = k$   
  
$x$ can be partitioned into protected and unprotected features
$$x = (x_{p}, x_{u})$$

## Assumptions
For each individual, there is a quantity that specifies target of prediction
$$y \in \{0,1\}$$  
  
Define random variables $X$ and $Y$ that take on values $X = x$ and $Y = y$ for an individual drawn randomly form population  
  
True risk function 
$$r(x) = Pr(Y = 1 | X = x)$$  
  
Risk assessment algorithms produce a risk score $s(x)$ that may be viewed as an approximation of true risk $r(x)$ which often has a threshold to convert from risk scores to decisions, setting $d(x) = 1$ if and only if $s(x) \geq t$ for some fixed threshold $t \in \mathbb{R}$

## COMPAS Data Overview
In 2016, ProPublica analyzed a commercially available algorithm made by Northpointe, Inc. that estimated the likelihood of a criminal defendant becoming a recidivist (i.e., re-offending). Here's a brief description of the data

> We looked at more than 10,000 criminal defendants in Broward County, Florida, and compared their predicted recidivism rates with the rate that actually occurred over a two-year period. When most defendants are booked in jail, they respond to a COMPAS questionnaire. Their answers are fed into the COMPAS software to generate several scores including predictions of “Risk of Recidivism” and “Risk of Violent Recidivism.”

We'll focus on the "Risk of Recidivism" score to apply the ideas in Corbett-Davies and Goel's work. The scores range from 1 (lowest risk) to 10 (highest risk). For a full description of the analysis, see [here](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm).

## Anti-Classification

:::::::::::::: {.columns}
::: {.column}
Anti-classification: a decision does not consider protected attributes  

$d(x) = d(x^{\prime})$ for all $x, x^{\prime}$ such that $x_{u} = x^{\prime}_{u}$

The COMPAS algorithm doesn't use race as an input, so a basic application of the risk score is essentially applying the "Anti-Classification" principle. Let's start with ROC curves for predicting recidivism for white and Black defendants.
:::
::: {.column}
```{r echo = FALSE, message = FALSE, warning = FALSE,  fig.width = 10}
library(tidyverse)
library(ROCR)

scores <- read.csv('../data/compas-scores-two-years.csv')

# filter to white and Black only
scoresWB <- scores %>% 
  filter(race %in% c('African-American', 'Caucasian'))

# groups 
g <- unique(as.character(scoresWB$race))
lineColors <- c('red','blue')

# list for group values
outPerf <- list()

z <- 1
for(i in g) {
  # filter 
  s <- scoresWB %>% filter(race == i)
  
  # create prediction/performance
  pred <- prediction(s$decile_score, s$two_year_recid)
  rocPerf <- performance(pred, 'tpr','fpr')
  acc <- performance(pred,'acc')
  tnr <- performance(pred,'tnr')
  fnr <- performance(pred,'fnr')
  ppv <- performance(pred,'ppv')
  npv <- performance(pred,'npv')
  
  
  
  # create dataframe of cutoffs, tpr, and fpr
  outPerf[[z]] <- data.frame(race = i,
                             cutoff  = rocPerf@alpha.values[[1]],
                             tpr = rocPerf@y.values[[1]],
                             fpr = rocPerf@x.values[[1]],
                             acc = acc@y.values[[1]],
                             tnr = tnr@y.values[[1]],
                             fnr = fnr@y.values[[1]],
                             ppv = ppv@y.values[[1]],
                             npv = npv@y.values[[1]])
  
  # if not first curve, add to existing plot
  b <- if(z > 1) {
    TRUE
  } else {
    FALSE
  }
  l <- if(z > 1){
    2
  } else {
    1
  }
  
  # plot
  plot(rocPerf, add = b, lwd = 3, lty = l, cex.axis = 2, cex.lab = 1.5, 
       mar = c(5,5,4,2), col = lineColors[z])
  text(x = 0.6, y = 0.5-(z*.1),cex = 1.5, labels = paste(g[z],' AUC: ', round(performance(pred, 'auc')@y.values[[1]],3)))
  
  if (z == 1) {
    legend('bottomright',
           legend = g,
           col = lineColors,
           lty = c(1,2))
  }
  
  z <- z + 1
} 
test <- performance(pred,'auc')

```
:::
::::::::::::::


## Anti-Classification Limitations
:::::::::::::: {.columns}
::: {.column}
If we look at error rates (false positive rate and false negative rate) by race, we see some discrepancies. At any given cutoff Caucasians have a higher false negative rate and African-Americans have a higher false positive rate.
:::
::: {.column}
```{r, echo = FALSE, message = FALSE, warning = FALSE,  fig.width = 12}
library(gridExtra)
# bind group dataframes 
perfDf <- do.call(rbind,outPerf)

# plot fpr comparing groups
p1 <- ggplot(data = perfDf, aes(x = as.factor(cutoff),
                                y = fpr, 
                                group = race,
                                linetype = race, 
                                color = race)) + 
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4)+
  theme(legend.position = 'none',
        text = element_text(size = 20)) +  
  ylab('False Positive Rate')+
  xlab('Model Cutoff')+
  ggtitle('False Pos. Rate by Cutoff')

# plot fnr comparing groups
p2 <- ggplot(data = perfDf, aes(x = as.factor(cutoff),
                                y = fnr, 
                                group = race,
                                linetype = race,
                                color = race)) +
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4)+
  theme(legend.position = c(.75,.15),
        text = element_text(size = 20))+
  ylab('False Negative Rate')+
  xlab('Model Cutoff')+
  ggtitle('False Neg. Rate by Cutoff')

grid.arrange(p1,p2, ncol = 2)
```
:::
::::::::::::::

## Anti-Classification
:::::::::::::: {.columns}
::: {.column}
But there are other metrics to consider in a confusion matrix. In Northpointe's rebuttal to ProPublica, they argue that COMPAS achieves predictive parity since the positive and negative predictive values are "comparable". We can use the ProPublica data to look at predictive parity across model cutoffs. 
    
As we will discuss, confusion matrix metrics on their own are not conclusive for a model's fairness, but rather a diagnostic starting point.
:::
::: {.column}
```{r,echo = FALSE, message = FALSE, warning = FALSE,  fig.width = 12}
# plot fpr comparing groups
p3 <- ggplot(data = perfDf, aes(x = as.factor(cutoff),
                                y = ppv, 
                                group = race,
                                linetype = race, 
                                color = race)) + 
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4)+
  theme(legend.position = 'none',
        text = element_text(size = 20)) +  
  ylab('Positive Predictive Value')+
  xlab('Model Cutoff')+
  ggtitle('Positive Predictive Value by Cutoff')+
  ylim(0,1)

# plot fnr comparing groups
p4 <- ggplot(data = perfDf, aes(x = as.factor(cutoff),
                                y = npv, 
                                group = race,
                                linetype = race,
                                color = race)) +
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4)+
  theme(legend.position = c(.25,.15),
        text = element_text(size = 20))+
  ylab('Negative Predictive Value')+
  xlab('Model Cutoff')+
  ggtitle('Negative Predictive Value by Cutoff') +
  ylim(0,1)

grid.arrange(p3,p4, ncol = 2)
```
:::
::::::::::::::
## Classification Parity
Classification parity: some given measure of classification error is equal across groups defined by protected attributes. There are lots of metrics from confusion matrix that could be used [here](https://journals.sagepub.com/doi/full/10.1177/0049124118782533), as well as the Area Under the Curve. Corbett-Davies et. al. highlight:  
  
Demographic parity: parity in proportion of positive decisions  
$$Pr(d(X) = 1 | X_{p}) = Pr(d(X) = 1)$$
  
Parity of false positive rates  
$$Pr(d(X)  = 1 | Y = 0, X_{p}) = Pr(d(X) = 1 | Y = 0)$$

## Classification Parity
:::::::::::::: {.columns}
::: {.column}
For a simple demonstration of classification parity, let's take the error curves from the prior example. If we shift the curves for African-Americans to the left by 2, they become very comparable at several risk scores to the curves for Caucasians. So by subtracting 2 from the risk scores for African-Americans, the model becomes "fairer" based on false positive and false negative rates according to classification parity... but there is more to it.
:::
::: {.column}
```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.width = 12}

perfDf <- perfDf %>%
  mutate(cutoff2 = ifelse(race == 'African-American',cutoff-2, cutoff))

# plot fpr comparing groups
p5 <- ggplot(data = perfDf, aes(x = as.factor(cutoff2), 
                                y = fpr, 
                                group = race, 
                                color = race,
                                linetype = race)) + 
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4)+
  theme(legend.position = 'none',
        text = element_text(size = 20)) +
  ylab('False Positive Rate') +
  xlab('Modified Model Cutoff') + 
  ggtitle('Classification Calibration: FPR')

# plot fnr comparing groups
p6<- ggplot(data = perfDf, aes(x = as.factor(cutoff2), 
                               y = fnr, 
                               group = race, 
                               color = race,
                               linetype = race)) + 
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4)+
  theme(legend.position = c(0.75,0.15),
        text = element_text(size = 20))+
  ylab('False Negative Rate') +
  xlab('Modified Model Cutoff') + 
  ggtitle('Classification Calibration: FNR')

grid.arrange(p5,p6, ncol = 2)
```
:::
::::::::::::::

## Limitations of Classification Parity
:::::::::::::: {.columns}
::: {.column}
Corbett-Davies et. al. point out that when your model does not meet classification parity (any kind), it does not *necessarily* mean your model is not fair. Rather, it says different groups have different estimated risk rates. Is it because:   
- These groups truly have different risk rates (this answer deserves high scrutiny!)  
- The data feeding into your model are not of equal quality across groups (back to Selection & Unconcious Bias)  
- The model hasn't properly captured the relationship between the features and risk  
:::
::: {.column}
![Estimated Risk Distributions. Source: [Corbett-Davies et. al. 2018](https://arxiv.org/pdf/1808.00023.pdf)](../images/violent-recid-dist.JPG)

:::
:::::::::::::: 

## Calibration
:::::::::::::: {.columns}
::: {.column}
Calibration: outcomes should be independent of protected attributes conditional on risk score. Given risk scores $s(x)$:
$$Pr(Y = 1 | s(X), X_{p}) = Pr(Y =1 | s(X))$$
At several model cutoffs, the chart below shows that the model is closely calibrated between white and Black defendants.
:::
::: {.column}
```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.width= 8}
cal <- scoresWB %>%
  group_by(decile_score, race) %>%
  summarize(n = n(),
         recid.count = sum(two_year_recid),
         recid.rate = recid.count / n )

ggplot(cal, aes(x = as.factor(decile_score), 
                y = recid.rate, 
                group = race, 
                linetype = race,
                color = race)) +
  geom_line(stat = 'identity', size = 1.5) +
  geom_point(size = 4) +
  ylab("Two Year Recidivism Rate")+
  xlab("Model Cutoff")+
  ggtitle("Recidivism Rates by Model Cutoff")+
  ylim(0,1) +
  theme(legend.position = c(0.75,0.2),
        text = element_text(size = 20))
```
:::
::::::::::::::

## Calibration Limitations
:::::::::::::: {.columns}
::: {.column}
Corbett-Davies et. al. present redlining as a perfect example of how you can satisfy calibration, but not achieve fairness. Imagine a scenario with two neighborhoods (1 & 2), each with two groups (A & B). Within each neighborhood, risk distributions are the same across groups. Neighborhood 1 is mostly Group B, and Neighborhood 2 mostly Group A. A loan classifier model is built exclusively on neighborhood. It rates all neighborhood 1 as "denied" and all neighborhood 2 as "approved." In this example, the model is ignored and everyone is given a loan. 
:::
::: {.column}
```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.width= 24, fig.height=8}
set.seed(33)
n<- 1000
s <- .1
d <- data.frame(group = rep(c('A','B','A','B'),c(n*s,n*(1-s),n*(1-s),n*s)), 
                neighborhood = rep(c('nh1','nh2'),c(n,n)),
                default.risk = c(round(rnorm(n*s,0.5,.12),2),
                                 round(rnorm(n*(1-s),0.5,.12),2),
                                 round(rnorm(n*(1-s),0.33,.12),2),
                                 round(rnorm(n*s,0.33,.12),2)),
                loan = rep(c('Denied','Approved'),c(n,n))
                )

g1 <- ggplot(d, aes(x = default.risk, fill = group))+
  geom_density(alpha = 0.5, aes(y = ..count..))+
  facet_grid(rows = vars(neighborhood))+
  xlim(0,1)+
  theme(legend.position = 'none',
        text = element_text(size = 32)) +
  ggtitle('Risk Distribution')

dSum <- d %>% 
  group_by(loan, group) %>%
  summarize(default.rate = mean(default.risk),
            n = n())

g2 <- ggplot(dSum, aes(x = loan, y = default.rate, fill = group)) +
  geom_bar(stat = 'identity', position = 'dodge')+
  theme(legend.position = c(0.15,0.85),        
        text = element_text(size = 32)) +
  ggtitle('Default Rates')


g3 <- ggplot(dSum, aes(x = loan, y = n, fill = group)) +
  geom_bar(stat = 'identity', position = 'dodge')+
  theme(legend.position = 'none',
        text = element_text(size = 32)) +
  ggtitle('Loan Status Counts')


grid.arrange(g1,g2,g3, ncol = 3)

```
:::
::::::::::::::


## Double-Edged Data Sword
In order to evaluate models for their impact on certain sub-groups, your data needs to contain observations of whatever variable defines these sub-groups. 

These data can be powerful and helpful when applied to making algorithms fairer, but they can also be used for nefarious discriminatory purposes. Always think carefully about:   
  
- What data are actually necessary  
  
- How the data are collected 
  
- Who can access the data  
  
- How the data could be misused

## Summary and Future Thoughts
One takeaway from this discussion is that quantifying fairness in machine learning is...complicated. But there are some other key points too:  
  
- Anytime you are working on a model, reflect and hypothesize with your team and clients on how it will impact people, and if it will impact different groups of people in different ways.  
  
- Test your hypotheses with existing quantitative tools, some of which we discussed today, and think about how you might invent your own. This is a new field with lots of room for improvement!  
  
- Think critically about the implications and limitations of these tests, discuss with your team and clients for richer dialogue and increased transparency.   


## Bibliography
Angwin, Julia, Jeff Larson, Surya Mattu and Lauren Kirchner. "Machine Bias." *ProPublica* May 23, 2016. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing (Accessed January 17, 2021)  
  
Atwood, James, Hansa Srinivasan, Yoni Halpern, D. Sculley. "Fair treatment allocations in social networks." *Fair ML for Health 2019*. https://arxiv.org/abs/1911.05489 (Accessed January 22, 2021)
  
Benjamin, Ruha. *Race After Technology: Abolitionist Tools for the New Jim Code.* Cambridge, UK: Polity Press, 2019.   
  
Berk, Richard, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. "Fairness in Criminal Justice Risk Assessments: The State of the Art." *Sociological Methods & Research* 50, no.1 (February 2021):3-44.https://journals.sagepub.com/doi/full/10.1177/0049124118782533 (Accessed January 25, 2021).
  
Bolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. "Man is to Computer Programmer as Woman is to Homemaker?: Debiased Word Embeddings." *NIPS '16: Proceedings of the 30th International Conference on Neural Information Processing Systems* (December 2016): 4356-4364. https://dl.acm.org/doi/10.5555/3157382.3157584 (Accessed January 17, 2021).

## Bibliography
Buolamwini, Joy and Timnit Gebru. "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification." *Proceedings of Machine Learning Research* 81 (2018): 77-91. http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf (Accessed January 17, 2021).   
  
Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. "Algorithmic Decision Making and the Cost of Fairness." *KDD '17: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining* (August 2017): 797-806. https://arxiv.org/abs/1701.08230 (Accesed January 24, 2021).   

Corbett-Davies, Sam and Sharad Goel. "The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning." (August 2018). https://arxiv.org/pdf/1808.00023.pdf (Accessed January 17, 2021).
  
D'Amour, Alexnader, Hansa Srinivasan, James Atwood, Pallavi Baljekar, D. Sculley, and Yoni Halpern. "Fairness Is No Static: Deeper Understanding of Long Term Fainress via Simulation Studies." *FAT '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency* (January 2020):525-534. https://dl.acm.org/doi/abs/10.1145/3351095.3372878 (Accessed January 22, 2021).  

## Bibliography 
Deitrich, William, Christina Mendoza, and Tim Brennan. "COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Equity." (July 2016). https://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf (Accessed January 24, 2021)  
  
Diaz, Fernando, Bhaskar Mitra, Michael D. Ekstrand, Asia J. Biega, and Ben Carterette. "Evaluating Stochastic Rankings with Expected Exposure." *Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM ’20)* (October 2020). https://doi.org/10.1145/3340531.3411962 (Accessed January 17, 2021).    
    
Du, Mengnan, Fan Yang, Na Zou, and Xia Hu. "Fairness in Deep Learning: A Computational Perspective." *IEEE Intelligent Systems* (June 2020). https://ieeexplore.ieee.org/document/9113719 (Accessed January 24, 2021).
  
Hutchinson, Ben and Margaret Mitchell. "50 Years of Testing (Un)Fairness: Lessons for Machine Learning." *FAT '19: Proceedings of the Conference on Fairness, Accountability, and Transparency* (January 2019): 49-58.https://arxiv.org/pdf/1811.10104.pdf (Accessed January 17, 2021).   

## Bibliography
Kirtichenko, Svetlana and Saif Mohammad. "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems." *Proceedings of the Seventh Join Conference on Lexical and Computational Semantics* (June 2018): 43-53. https://www.aclweb.org/anthology/S18-2005/ (Accessed January 24, 2021).  
  
Larson, Jeff, Surya Mattu, Lauren Kirchner, and Julia Angwin. "How We Analyzed the COMPAS Recidivism Algorithm." *ProPublica* May 23, 2016. https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm (Accessed January 17, 2021)  
  
Noble, Safiya Umoja. *Algorithms of Opression: How Search Engines Reinforce Racism.* New York: New York University Press, 2018.  
  
O'Neil, Cathy. *Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy.* New York: Crown Publishing Group, 2016.  

## Bibliography
Sun, Tony, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei chang, and William Yang Wang. "Mitigating Gender Bias in Natural Language Processing: Literature Review." *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics* (2019): 1630-1640. https://www.aclweb.org/anthology/P19-1159.pdf (Accessed January 17, 2021).    

Yeo, Catherine and Alyssa Chen. "Defining and Evaluating Fair Natural Language Generation." *Proceedings of the Fourth Widening Natural Language Processing Workshop* (July 2020). https://www.aclweb.org/anthology/2020.winlp-1.27/ (Accessed January 17, 2021).  

